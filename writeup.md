#**Traffic Sign Recognition** 

---

**Build a Traffic Sign Recognition Project**

The goals / steps of this project are the following:
* Load the data set (see below for links to the project data set)
* Explore, summarize and visualize the data set
* Design, train and test a model architecture
* Use the model to make predictions on new images
* Analyze the softmax probabilities of the new images
* Summarize the results with a written report

## Rubric Points
###Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation.  

---
###Writeup / README

####1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. You can use this template as a guide for writing the report. The submission includes the project code.

You're reading it! and here is a link to my [carnd-term1-p2](https://github.com/antalakas/carnd-term1-p2/blob/master/P2.ipynb)

###Data Set Summary & Exploration

Initially, all imports are declared and the signames.csv file is loaded for later use

####1. Provide a basic summary of the data set and identify where in your code the summary was done. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.

The code for this step is contained in the third code cell of the IPython notebook.  

I used the numpy library to calculate summary statistics of the traffic
signs data set:

* The size of training set is 34799
* The size of the validation set is 4410
* The size of test set is 12630
* The shape of a traffic sign image is (32, 32)
* The number of unique classes/labels in the data set is 43

####2. Include an exploratory visualization of the dataset and identify where the code is in your code file.

The code for this step is contained in the fourth code cell of the IPython notebook.  

Here is an exploratory visualization of the data set. It is a bar chart showing how the training data is distributed acroos classes

![Training examples per class][https://github.com/antalakas/carnd-term1-p2/blob/master/training_examples_per_class.png]

Please also note that the visualization includes sample (10) images per class along with their class name

###Design and Test a Model Architecture

####1. Describe how, and identify where in your code, you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc.

The code for this step is contained in the code cell 5-6 of the IPython notebook.

I normalized the image data scaling in the range of [0, 1] to keep numerical stability for any of the larger mathematical sequences that might be occurring.

The images were converted to grayscale, having in mind that i was going to apply the LeNet architecture as a starting point, as i am rather inexperienced with convnets.

LeNet architecture, accepts grayscaled images

As mentioned in this [paper](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf), using color channels didnâ€™t seem to improve things a lot.

Also, localized histogram equalization has been applied, as it seems to improve feature extraction even further in our case.

####2. Describe how, and identify where in your code, you set up training, validation and testing data. How much data was in each set? Explain what techniques were used to split the data into these sets. (OPTIONAL: As described in the "Stand Out Suggestions" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, identify where in your code, and provide example images of the additional data)

The initial pickled image data set came with a validation set already splitted from the training set and in the Part 1 of this discussion it was found to contain 4410 images.

Validation and testing set also undergo the preprocessing.

The code cells [7, 10] of the IPython notebook contain the code for augmenting the data set. I decided to generate additional data because from the previous visualization it seems that the dataset is very unbalanced, and some classes are represented significantly better than others.
To add more data to the the data set, I used the following techniques:

* The mean number of images per class was found.
* For each class that the number od images was under the mean threshold, new images were generated by rotating the original image in random, predefined angle
* Some traffic signs are invariant to horizontal and/or vertical flipping, which basically means that we can flip an image and it should still be classified as belonging to the same class.  

The difference between the original data set and the augmented data set is the following (more than double):

Size of training set is 34799
Size of the validation set is 4410
Number of training examples after augmentation and horizontal flipping = 78229
Number of validation examples after augmentation and horizontal flipping = 9492

####3. Describe, and identify where in your code, what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.

The code for my final model is located in the cells [11, 12] of the ipython notebook. 

My final model consisted of the following layers:

| Layer         		|     Description	        					| 
|:---------------------:|:---------------------------------------------:| 
| Input         		| 32x32x1 RGB image   							| 
| Convolution 5x5     	| 1x1 stride, VALID padding, outputs 28x28x6 	|
| RELU					|												|
| Max pooling	      	| 2x2 stride,  outputs 14x14x6 				    |
| Convolution 5x5     	| 1x1 stride, VALID padding, outputs 10x10x16 	|
| RELU					|												|
| Max pooling	      	| 2x2 stride, outputs 5x5x16 				    |
| Flatten		        | outputs 400       							|
| Fully connected		| outputs 200       							|
| Dropout		        | keep prob: 0.7     							|
| Fully connected		| outputs 100       							|
| Dropout		        | keep prob: 0.7     							|
| Fully connected		| outputs 43           							|
| Softmax				|            									|
| Reduce Mean Loss op   |												|
|:---------------------:|:---------------------------------------------:| 

####4. Describe how, and identify where in your code, you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.

The code for training the model is located in the cells [11-14] of the ipython notebook. 

To train the model, the following parameters were used:
* Learning rate: 0.001
* Turn labels into unit row vector applying One-Hot Encoding
* Mean for initial weights: 0
* Standard deviation for initial weights: 0.1
* Batch size: 150
* Number of EPOCHS: 120

####5. Describe the approach taken for finding a solution. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.

The code for calculating the accuracy of the model is located in the ninth cell of the Ipython notebook.

My final model results were:
* training set accuracy of 1.000
* validation set accuracy of 0.977
* test set accuracy of 0.9584328532218933

Initially i used the LeNet architecture, as described in the lesson, without any data preprocessing, because it was the obvious choice. 
I experimented with augmentation, grayscale and normalization in the range of [0.1, 0.9] (as described here: [mvirgo](https://github.com/mvirgo/Traffic-Sign-Classifier/blob/master/Traffic_Sign_Classifier.ipynb)).
I experimented with ideas found here: [navoshta](http://navoshta.com/traffic-signs-classification/), that led to the current solution.
After combining both solutions. here are my findings:

* Original LeNet led to validation accuracy of less than 0.93
* Training accuracy was always over 0.99
* I had to correct the overfitting closing the gap between validation and training set accuracies
* Normalization in the range [0, 1] gave better results than [0.1, 0.9]
* Dropout of 0.7 after the first two fully connected layers used in combination with the LeNet as described by [mvirgo] gave good results and performsmuch faster than deeper network described by [navoshta].
* Usage of localized histogram localization led to better results.
* Data augmentation and the addition of new images using flipping, had good results.
* Probably, if a technique for augmentation included skewing except fro rotation, the results would have been even better. 
* The neural network, really needs a lot of data to train, the more augmented data produced, the better.
* The more balanced the dataset, the better the results.
* Interestingly enough, between different runs, the validation accuracy does not change, but the testing does. As a result, for new images, there are examples of different class interpretation based on different trained weights.
* Maybe a technique of "adding" up the knowledge between different trainins, would improve the accuracy overall.
 

###Test a Model on New Images

####1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.

43 new images are used to test the accuracy on new images. They can be found [here](https://github.com/antalakas/carnd-term1-p2/tree/master/additional_images)
38 of them are included by [navoshta], 5 of them are included from [mvirgo].

####2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. Identify where in your code predictions were made. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the "Stand Out Suggestions" part of the rubric).

The code for making predictions on my final model is located in the cells [15-20] of the Ipython notebook.

In cell [19] there can be found a bar chart for each image describing the top 5 probabilities for the image, along with the images itself and the preprocessed image.

The model was able to correctly guess 34 of the 40 traffic signs, which gives an accuracy of 85.0%. Far from perfect but the new images set contain some German signs, along with UK signs and non-EU signs.

Three new images had no relevant classes.

####3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction and identify where in your code softmax probabilities were outputted. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the "Stand Out Suggestions" part of the rubric, visualizations can also be provided such as bar charts)

The code for making predictions on my final model is located in the [21] cell of the Ipython notebook. 
The top five soft max probabilities were also printed. 
The previous section containing the bar charts give more intuitive overview.

The 11th image is misinterpreted as Bicycles crossing (correct: Children crossing), but as we can see from the visualization, indeed the processed image is unclear.
The 17th image is a surprise for me, the speed limit (80) is completely different from roundabout mandatory, while we have many correct predictions for roundabouts.
The 25th image is interesting: While its class is not existing, it is interpreted as Road narrows on the right and it seems like it.

####4. Visualize layers of the neural network
The code for visualizing the layers of the neural network is located in the cells [22-25] of the Ipython notebook.
For the double curve image from the new image dataset, the first convolutional layer clearly tries to figur out the general sign shape (the triangle, the inner rotated "S")
The second layer tries to identify fine details like the edges of the triangle and the inner rotated "S" line.